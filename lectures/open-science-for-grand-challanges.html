<section>
<h2>Open Science</h2>
<h3 style="margin-top: 0.5em">
    Vaclav (Vashek) Petras</h3>
<p class="title-foot">
    <a href="http://www.ncsu.edu/" title="North Carolina State University">NCSU</a>
    <a href="http://geospatial.ncsu.edu/osgeorel/" title="NCSU GeoForAll Lab">GeoForAll Lab</a>
    at the
    <a href="http://geospatial.ncsu.edu/" title="Center for Geospatial Analytics">Center for Geospatial Analytics</a>
    <br>
    North Carolina State University
</p>
<p>
    GIS 710: Geospatial Analytics for Grand Challenges<br>
    November 12, 2018
</p>
</section>


<section>
    <h3>Reproducibility of Computational Articles</h3>
    <p>
    Stodden et al. (PNAS, March 13, 2018)<br>
    204 computational articles from Science in 2011–2012
    </p>
    <img class="stretch" style="margin: 0px"; src="img/reproducibility_ratio_from_stodden.png" alt="26% reproducible, 74% irreproducible">
    <small class="credit">
    Stodden, V., Seiler, J., &amp; Ma, Z. (2018).
    An empirical analysis of journal policy effectiveness for computational reproducibility.
    In: <em>Proceedings of the National Academy of Sciences</em>
    115(11), p. 2584-2589.
    <a href="https://doi.org/10.1073/pnas.1708290115">DOI&nbsp;10.1073/pnas.1708290115</a>
    </small>

    <small class="glow">
        Discussion questions:
        Do you know about similar studies?
        What do they say?
    </small>

    <aside class="notes">
        A recently published study showed that 74% of computational
        articles published in Science was not computationally reproducible.
        This brings questions about validity of findings in these
        articles and also about the peer-review process.
        <!--
        Are these articles useful even to read when we don't know how
        the findings were obtained? ...let alone applying the findings.
        -->
    </aside>
</section>

<!--
Stodden et al. (2018):
We were able to obtain data and code from the authors of 89
articles in our sample of 204, giving an estimate for the artifact
recovery rate of 44% for articles published in Science shortly
after the policy change: (65 + 24/204) with a 95% bootstrap
confidence interval of [0.36, 0.50]. Of the 56 articles that were
then deemed potentially reproducible, we randomly chose 22
to attempt replication, and all but 1 of the 22 provided enough
information that we were able to reproduce their computational
findings (given sufficient resources and a willingness write some
code). We estimate 95% (21/22) of the articles deemed reproducible
by inspection are computationally reproducible, so for
the full sample, we estimate 26% will computationally reproduce
((56 ∗ (1 − 1/22))) with a 95% bootstrap confidence interval for
the proportion [0.20, 0.32].
((56 * (1 - 1./22)) / 204)
-->


<section>
<h3>Open Science Beginnings</h3>

<p>
First journal ever published:<br>
<em>Philosophical Transactions (of the Royal Society)</em>
</p>

<img src="img/philosophical_transactions.jpg" class="stretch">
<p class="credit glow">
    CC BY Stefan Janusz,
    <a href="https://en.wikipedia.org/wiki/File:Philosophical_Transactions_Volume_1_frontispiece.jpg">Wikipedia</a>
</p>
</section>


<section>
<h3>Publishing Goals</h3>
<ul>
    <li>registration <small>so that scientists get credit</small>
    <li>archiving <small>so that we preserve knowledge for the future</small>
    <li>dissemination <small>so that people can use this knowledge</small>
    <li>peer review <small>so that we know it's worth it</small>
</ul>

<p>
<small>
    Discussion questions:
    Do you agree with these publishing goals?
    How are these different from goals of science?
    Are these publishing goals fulfilled by journal papers?
</small>
</section>


<section>
<h3>Open Science</h3>

<img src="img/spectrum_of_reproducible_research.png" class="stretch">
<small>[Buckheit and Donoho 1995, Peng 2011, Rodríguez-Sánchez et al. 2016, Marwick 2016]</small>
<p class="credit">
    Image credit: CC BY-SA Comtebenoit,
    <a href="https://commons.wikimedia.org/wiki/File:Spectrum_of_reproducible_research.png">Wikimedia</a>
    <a href="https://commons.wikimedia.org/wiki/Commons:Undeletion_requests/Archive/2017-05#File:Reproducibility_Spectrum.png">(note: file freedom disputed)</a>
<!--
Peng, R. D. (2011). Reproducible research in computational science.
Science, 334(6060), 1226-1227.
DOI: 10.1126/science.1213847

Rodríguez-Sánchez, F., Bartomeus, I., Varela, S., Pérez-Luque, A.J.,
Ciencia reproducible: qué, por qué, cómoEcosistemas [en linea] 2016, 25
(Mayo-Agosto) : [Fecha de consulta: 4 de abril de 2017]
Disponible en:<http://www.redalyc.org/articulo.oa?id=54046745011>
ISSN 1132-6344

Marwick, B. (2016). Computational reproducibility in archaeological research:
basic principles and a case study of their implementation.
Journal of Archaeological Method and Theory, 1-27.
DOI: 10.1007/s10816-015-9272-9

There was "significant doubt about the freedom of a particular file"
(https://commons.wikimedia.org/wiki/Commons:Undeletion_requests/Archive/2017-05#File:Reproducibility_Spectrum.png)
but the references (especially with those added here) are present
(covering the line of authors of the idea and the image),
there was an effort and intention from authors to make things CC,
and uncopyrightable concepts were used when creating the image,
so leaving it here as is.
-->
</p>

<p>
<small>
    Discussion questions:
    Do you agree with this spectrum?
    On which side of the spectrum have you published?
</small>

</section>


<section>
<h3>Open Science Components</h3>

<ul class="left">
<li>6 pillars <small>[Watson 2015]</small>:
    <ul>
        <li>open methodology
        <li>open access
        <li>open data
        <li>open source
        <li>open peer review
        <li>open education (or educational resources)
    </ul>
<li>other components:
    <ul>
        <li>open hardware
        <li>open formats
        <li>open standards
    </ul>
</ul>

<ul class="right">
<li>related concepts:
    <ul>
        <li>Open-notebook science
        <li>Science 2.0 (like Web 2.0)
        <li>Citizen science
        <li>Public science
        <li>Participatory research
        <li>Open innovation
        <li>Crowdsourcing
        <li>Preprints
        <li>Inner source
    </ul>
</ul>

<p>
<small>
    Discussion questions:
    What would add to the list?
    Do you see something for the first time?
</small>

</section>

<!--
Watson, M. (2015). When will ‘open science’ become simply ‘science’?.
Genome biology, 16(1), 101. doi:10.1186/s13059-015-0669-2
There are six commonly accepted pillars of open science: open data,
open access, open methodology, open source, open peer review and open education.
-->


<section>
<h3>The “re” Words</h3>

No agreement on some of the definitions especially in different fields;
definitions are often overlapping or swapped, some don't make any distinction.

<ul>
    <li>replicability <small>independent validation of specific findings</small>
    <li>repeatability <small>same conditions, people, instruments, ... (test–retest reliability)</small>
    <li>recomputability <small>same results in computational research</small>
    <li>reproducibility <small>obtain again same results from the raw data</small>
    <li>reusability <small>use again the same data or methods</small>
</ul>

<small>
For example, Ince et al. (2012) in computational science
distinguishes direct reproducibility as rerunning of the code and
indirect reproducibility as validate something other than the entire code.

</small>

<p>
<small>
    Discussion questions:
    How do want science to look like?
    Are there some minimal requirements?
    What should be possible or easy to do for you
    when you receive a scientific publication (e.g. for review)?
</small>

</section>


<section>
<h3>Internal Reasons for Open Science</h3>

Internal or selfish reasons for doing open science

<ul>
    <li>in your lab:
        <ul>
            <li>collaboration <small>work together with your colleagues</small>
            <li>transfer <small>transfer research between you and your colleague</small>
        </ul>
    <li>yourself:
        <ul>
            <li>revisit <small>revisit or return to a project after some time</small>
            <li>correction <small>correct a mistake in the research</small>
            <li>extension <small>improve or build on an existing project</small>
        </ul>
</ul>

<p>
<small>
    Discussion questions:
    What is your experience with getting back to your own research
    or continuing research started by someone else?
    <small>(See <a href="http://phdcomics.com/comics.php?f=1689">PhD Comics: Scratch</a>.)</small>
</small>

</section>


<section>
<h3>What Open Means</h3>

<!-- TODO: links -->
<ul>
    <li>open, free, libre
    <li>The Open Definition
        <ul>
            <li>Open means anyone can freely access, use, modify, and share
                for any purpose
                (subject, at most, to requirements that preserve
                provenance and openness).
                <small>[as officially summed up]</small>
        </ul>
    <li>Free Cultural Works
    <li>The Open Source Definition
    <li>The Free Software Definition
    <li>the Debian Free Software Guidelines and the Debian Social Contract
</ul>

<p>
<small>
    Discussion questions:
    What is the difference between “free as in free beer” and “free as in freedom”?
    Have you seen “open” being used for something not fulfilling the definition?
</small>

</section>


<section>
<h3>Licensing</h3>

<ul class="left">
    <li>Creative Commons (CC) licenses
        <ul>
            <li>(levels of openness)
        </ul>
    <li>GNU licenses
        <ul>
            <li>e.g. GNU General Public License (GPL)
        </ul>
    <li>BSD licenses
        <ul>
            <li>2-Clause and 3-Clause BSD
        </ul>
    <li>...
    <li>Lists of licenses and choosing a license:
        <ul>
            <li><a href="https://opensource.org/licenses">Open Source Initiative</a>
            <li><a href="https://www.gnu.org/licenses/license-list.html">GNU (Free Software Foundation)</a>
            <li><a href="https://choosealicense.com/">choosealicense.com</a> (for software)
            <li><a href="https://creativecommons.org/choose/">creativecommons.org/choose</a>
        </ul>
</ul>

<div class="right">
<img style="max-height: 15em !important;" src="https://creativecommons.org/wp-content/uploads/2013/09/spectrum-with0.png">
<p class="credit">
    CC BY 4.0
    <a href="https://creativecommons.org/share-your-work/public-domain/freeworks">Creative Commons</a>
</p>
</div>

<p>
<small>
    Discussion questions:
    Do you read “terms and conditions”?
    Have you ever read any “terms and conditions” or end user license agreement (EULA)?
    What about an open source software license?
    <small>(Read <a href="https://trac.osgeo.org/gdal/wiki/FAQGeneral#WhatlicensedoesGDALOGRuse">license of GDAL</a> right now!)</small>
</small>

</section>



<section>
<h3>Computational and Geospatial Research</h3>

<ul>
    <li>
        code part of method description
        <small>[Ince et al. 2012, Morin et al. 2012, Nature Methods 2007]</small>
    <li>
        use of open source tools is part of reproducibility
        <small>[Lees 2012, Alsberg &amp; Hagen 2006]</small>
    <li>
        <em>easily reproducible result</em> is a result obtained in 10 minutes
        <small>[Schwab et al. 2000]</small>
    <li>
        geospatial research specifics:
        <ul>
            <li>some research introduces new code
            <li>some research requires significant dependencies
        </ul>
</ul>

<p>
<small>
    Discussion questions:
    Is spatial special?
    Is recomputing the results useful for research?
    Should dependencies be open source as well?
    How long should it take to recompute results?
</small>

</section>


<section>
    <h3>Open Science Publication: Use Case</h3>
    Petras et al. 2017
    <br>
    <img class="stretch" src="img/petras2017generalized_full.png">
    <br>
    <small class="credit">
        Petras, V., Newcomb, D. J., &amp; Mitasova, H. (2017).
        <em>Generalized 3D fragmentation index derived from lidar point clouds.</em>
        In: Open Geospatial Data, Software and Standards 2(1), 9.
        <a href="https://doi.org/10.1186/s40965-017-0021-8">DOI 10.1186/s40965-017-0021-8</a>
    </small>
</section>


<section>
<h3>Open Science Publication: Components</h3>
<table style="font-size: smaller;">
    <tr>
        <th>Publication Component</th>
        <th>in the Petras et al. 2017 use case</th>
    <tr>
        <td>Text</td>
        <td>
            background, methods, results, discussion, conclusions, &hellip; <small>(OA)</small>
        </td>
    </tr>
    <tr>
        <td>Data</td>
        <td>
            input data <small>(formats readable by open source software)</small>
        </td>
    </tr>
    <tr>
        <td>Reusable&nbsp;Code</td>
        <td>
            methods as GRASS GIS modules <small>(C &amp; Python)</small>
        </td>
    </tr>
    <tr>
        <td>Publication-specific&nbsp;Code</td>
        <td>
            scripts to generate results <small>(Bash &amp Python)</small>
        </td>
    </tr>
    <tr>
        <td>Computational Environment</td>
        <td>
            details about all dependencies and the code <small>(Docker, Dockerfile*)</small>
        </td>
    </tr>
    <tr>
        <td>Versions</td>
        <td>
            repository with current and previous versions** <small>(Git, GitHub)</small>
        </td>
    </tr>
</table>
<p>
    <small style="font-size: 70%;">
    * Now also available at <a href="https://codeocean.com/2018/04/21/generalized-3d-fragmentation-index-derived-from-lidar-point-clouds/interface" title="Goes directly to the capsule interface">Code Ocean</a>.
    <br>
    ** Version associated with the publication included also as a supplemental file.
    </small>
</p>

<small class="credit">
Petras, V. (2018). <em>Geospatial analytics for point clouds
in an open science framework</em>. Doctoral dissertation.
<a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">URI&nbsp;http://www.lib.ncsu.edu/resolver/1840.20/35242</a>
</small>

<p>
<small>
    Discussion questions:
    What are other technologies which are good fit for these components?
    Are there other components or categories?
</small>

</section>


<section>
    <h3>Open Science Publication: Software Platform</h3>
    <ul>
        <li>Preprocessing, visualization, and interfaces <small>(GUI, CLI, API)</small>
        <li>Data inputs and outputs, memory management
        <li>Integration with existing analytical tools
        <li>Preservation of the reusable code component <small>(long-term maintenance)</small>
        <li>Dependency which would be hard to change for something else
        <li>Example: FUTURES model implemented as a set of GRASS GIS modules
            <small>(<em>r.futures.pga, r.futures.demand, r.futures.parallelpga, ...</em>)</small>
    </ul>

    <p class="glow">
    <small>
        Discussion questions:
        Discussion questions:
        What software can play this role?
        What are the different levels of integration with a piece of software?
    </small>
</section>


<section>
<h3>References</h3>

<small style="font-size: 50%" class="glow">

<ul>
<li>
Alsberg, Bjørn K., and Ole Jacob Hagen. “How Octave Can Replace Matlab in Chemometrics.” Chemometrics and Intelligent Laboratory Systems, Selected papers presented at the 9th Scandinavian Symposium on Chemometrics Reykjavik, Iceland 21–25 August 2005, 84, no. 1 (December 1, 2006): 195–200. <a href="https://doi.org/10.1016/j.chemolab.2006.04.025">doi:10.1016/j.chemolab.2006.04.025</a>.
<li>
Buckheit, Jonathan B., and David L. Donoho. “WaveLab and Reproducible Research.” In Wavelets and Statistics, edited by Anestis Antoniadis and Georges Oppenheim, 103:55–81. Lecture Notes in Statistics. New York, NY: Springer New York, 1995. <a href="https://doi.org/10.1007/978-1-4612-2544-7_5">doi:10.1007/978-1-4612-2544-7_5</a>.
<li>
Ince, Darrel C., Leslie Hatton, and John Graham-Cumming. “The case for open computer programs”. In: Nature 482.7386 (2012), pp. 485–488. <a href="http://doi.org/10.1038/nature10836">doi:10.1038/nature10836</a>
<li>
Lees, Jonathan M. “Open and free: Software and scientific reproducibility”. In: Seismological Research Letters 83.5 (2012), pp. 751–752. <a href="http://doi.org/10.1785/0220120091">doi:10.1007/s10816-015-9272-9</a>
<li>
Marwick, Ben. “Computational reproducibility in archaeological research: basic principles and a case study of their implementation”. In: Journal of Archaeological Method and Theory 24.2 (2017), pp. 424–450. <a href="http://doi.org/10.1007/s10816-015-9272-9">doi:10.1007/s10816-015-9272-9</a>
<li>
Morin, A et al. “Shining light into black boxes”. In: Science 336.6078 (2012), pp. 159–160. <a href="http://doi.org/10.1126/science.1218263">doi:10.1126/science.1218263</a>
<li>
Nature Publishing Group. “Social Software.” Nature Methods 4, no. 3 (March 2007): 189. <a href="https://doi.org/10.1038/nmeth0307-189">doi:10.1038/nmeth0307-189</a>.
<li>
Peng, Roger D. “Reproducible Research in Computational Science.” Science (New York, N.Y.) 334, no. 6060 (December 2, 2011): 1226–27. <a href="https://doi.org/10.1126/science.1213847">doi:10.1126/science.1213847</a>
<li>
Petras, Vaclav. “Geospatial analytics for point clouds in an open science framework.” Doctoral dissertation. 2018. <a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">www.lib.ncsu.edu/resolver/1840.20/35242</a>
<li>
Petras, Vaclav, Douglas J. Newcomb, and Helena Mitasova. “Generalized 3D Fragmentation Index Derived from Lidar Point Clouds.” Open Geospatial Data, Software and Standards 2, no. 1 (April 2017): 9. <a href="https://doi.org/10.1186/s40965-017-0021-8">doi:10.1186/s40965-017-0021-8</a>.
<li>
Rocchini, Duccio and Markus Neteler. “Let the four freedoms paradigm apply to ecology”. In: Trends in Ecology and Evolution (2012). <a href="http://doi.org/10.1016/j.tree.2012.03.009">doi:10.1016/j.tree.2012.03.009</a>
<li>
Rodriguez-Sanchez, Francisco, Antonio Jesús Pérez-Luque, Ignasi Bartomeus, and Sara Varela. “Ciencia Reproducible: Qué, Por Qué, Cómo.” Revista Ecosistemas 25, no. 2 (2016): 83–92.
<li>
Schwab, Matthias, Martin Karrenbach, and Jon Claerbout. “Making Scientific Computations Reproducible.” Computing in Science & Engineering 2, no. 6 (2000): 61–67. <a href="https://doi.org/10.1109/5992.881708">doi:10.1109/5992.881708</a>.
<li>
Stodden, V., Seiler, J., &amp; Ma, Z. (2018). “An empirical analysis of journal policy effectiveness for computational reproducibility.” In: <em>Proceedings of the National Academy of Sciences</em> 115(11), p. 2584-2589. <a href="https://doi.org/10.1073/pnas.1708290115">doi:10.1073/pnas.1708290115</a>
<li>
Watson, M. (2015). When will ‘open science’ become simply ‘science’?. Genome biology, 16(1), 101. <a href="https://doi.org/10.1186/s13059-015-0669-2">doi:10.1186/s13059-015-0669-2</a>
</ul>

</small>

</section>
