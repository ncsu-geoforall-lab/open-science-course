<!doctype html>
<html lang="en">
<!-- This is a generated file. Do not edit. -->
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <title>Tools for open geospatial science</title>

        <meta name="description" content="NCSU GIS 595-601: Tools for open geospatial science">
        <meta name="author" content="Vaclav Petras">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="shortcut icon" href="../img/favicon.png" />

        <link rel="stylesheet" href="css/reveal.css">
        <link rel="stylesheet" href="css/theme/ncsu-geoforall-lab.css" id="theme">

        <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/zenburn.css">
        <!-- For chalkboard plugin -->
        <link rel="stylesheet" href="css/font-awesome.min.css">

        <!-- If the query includes 'print-pdf', include the PDF print sheet -->
        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->

        <style>
            body {
                background-image: url("../img/header.svg");
                background-repeat: repeat-x;
                background-position: left bottom;
                background-size: auto 10%;
            }

            .glow {
                text-shadow: 0 0 20px white, 0 0 18px white, 0 0 16px white, 0 0 14px white, 0 0 12px white, 0 0 10px white, 0 0 8px white, 0 0 6px white;
            }

            .reveal .box-glow {
                box-shadow: 0px 0px 15px 0px white, 0px 0px 10px 0px white, 0px 0px 5px 0px white;
            }

            /*
            table {
                margin: 10px;
                border-collapse: separate;
                border: none;
            }

            td {
                display: inline-block;
                border-radius: 5px;
                background-color: #369;
                color: green;
                border: 5px solid red;
            }*/

            .reveal .row-based td {
                border: none;
            }

            .row-based tr td:first-of-type {
                font-weight: bold;
            }
        </style>
    </head>

    <body>

        <div class="reveal">

            <!-- Any section element inside of this container is displayed as a slide -->
            <div class="slides">
<section>
<h2>Open Science</h2>
<h3 style="margin-top: 0.5em">
    Vaclav (Vashek) Petras</h3>
<p class="title-foot">
    <a href="http://geospatial.ncsu.edu/osgeorel/">NCSU GeoForAll Lab</a>
    at the
    <a href="http://geospatial.ncsu.edu/">Center for Geospatial Analytics</a>
    <br>
    <a href="http://www.ncsu.edu/">North Carolina State University</a>
</p>
<img style="width: 25%;" src="img/logos/ncstate.png">
<p>
    GIS 710: Geospatial Analytics for Grand Challenges<br>
    October 27, 2025
</p>
<p>
    <a href="../topics/open-science-for-grand-challenges.html" style="font-size: 150%;">https://bit.ly/2oR9owy</a>
</p>
</section>


<section>
    <h3>Learning Objectives</h3>
    <ul>
        <li>Understating motivation for practicing open science
        <li>Understating openness
        <li>Understating complexity of practicing open science
        <li>Critical thinking about pros, cons, and challenges
        <li>General understanding of tools and services involved
        <li>Practical knowledge of tools for sharing research and computations
        <li>Ideas about how to use them in complex geospatial applications
        <li>Center's open projects and approach
    </ul>
</section>


<section>
    <h3>Vaclav (Vashek) Petras</h3>
    <ul>
        <li>Center: Director of Technology
        <li>GRASS: Core Development Team, Project Steering Committee
        <li>OSGeo: Charter Member</li>
    </ul>
</section>


<section>
    <h3>Reproducibility of Computational Articles</h3>
    <p>
    Stodden et al. (PNAS, March 13, 2018)<br>
    204 computational articles from <em>Science</em> in 2011–2012
    </p>
    <img class="stretch" style="margin: 0px"; src="img/reproducibility_ratio_from_stodden.png" alt="26% reproducible, 74% irreproducible">
    <small class="credit">
    Stodden, V., Seiler, J., &amp; Ma, Z. (2018).
    An empirical analysis of journal policy effectiveness for computational reproducibility.
    In: <em>Proceedings of the National Academy of Sciences</em>
    115(11), p. 2584-2589.
    <a href="https://doi.org/10.1073/pnas.1708290115">DOI&nbsp;10.1073/pnas.1708290115</a>
    </small>
    <p>
        <small>
            Ostermann, F.O., Nüst D., Granell, C., Hofer, B., Konkol, M. (2020): &lt;15% articles had <em>available</em> materials for <em>Methods</em> and <em>Results</em>
            <small>(75 articles from GIScience conference, 2012-2018)</small>
        </small>
    </p>

    <small class="glow">
        Discussion questions:
        Do you know about similar studies?
        What do they say?
    </small>

    <aside class="notes">
        A 2018 study showed that 74% of computational
        articles published in Science was not computationally reproducible.
        This brings questions about validity of findings in these
        articles and also about the peer-review process.
        <!--
        Are these articles useful even to read when we don't know how
        the findings were obtained? ...let alone applying the findings.
        -->
    </aside>
</section>

<!--
Stodden et al. (2018):
We were able to obtain data and code from the authors of 89
articles in our sample of 204, giving an estimate for the artifact
recovery rate of 44% for articles published in Science shortly
after the policy change: (65 + 24/204) with a 95% bootstrap
confidence interval of [0.36, 0.50]. Of the 56 articles that were
then deemed potentially reproducible, we randomly chose 22
to attempt replication, and all but 1 of the 22 provided enough
information that we were able to reproduce their computational
findings (given sufficient resources and a willingness write some
code). We estimate 95% (21/22) of the articles deemed reproducible
by inspection are computationally reproducible, so for
the full sample, we estimate 26% will computationally reproduce
((56 ∗ (1 − 1/22))) with a 95% bootstrap confidence interval for
the proportion [0.20, 0.32].
((56 * (1 - 1./22)) / 204)
-->


<section>
    <h3>What Gets into Papers?</h3>

    <blockquote>
        <em>More than 20% of chemistry researchers have deliberately added information they believe to be incorrect into their manuscripts during the peer review process, in order to get their papers published.</em>
        <small>
            <br>
            &mdash;
            1 in 5 chemists have deliberately added errors into their papers during peer review, study finds.
            <a href="https://cen.acs.org/policy/publishing/One-five-chemists-deliberately-added/103/web/2025/10">Chemical & Engineering News (C&EN)</a>.
            October 20, 2025.
            Access Date: 2025-10-24
        </small>
    </blockquote>

    <small class="glow">
    </small>
</section>


<section>
    <h2>Openness</h2>
</section>


<section>
    <h3>What Open Means</h3>

    <!-- TODO: links -->
    <ul>
        <li>The Open Definition
            <ul>
                <li>Knowledge is open if anyone is free to access, use, modify,
                    and share it — subject, at most, to measures
                    that preserve provenance and openness.
                    <small>[as officially summed up]</small>
            </ul>
        <li>More than one term in use: <em>open, free, libre, FOSS, FLOSS</em>
        <li>Free Cultural Works
        <li>The Open Source Definition
        <li>The Open Source AI Definition
        <li>The Free Software Definition
        <li>The Debian Free Software Guidelines and the Debian Social Contract
    </ul>

    <br>
    <img class="stretch" src="img/free_beer_bottles.jpg">
    <p class="credit">
        <small>
            Image: “Free beer bottles” by <a href="https://en.wikipedia.org/wiki/File:Free_beer_bottles.jpg">free beer pool</a> (CC BY 2.0)
        </small>
    </p>

    <p>
    <small class="glow">
        Discussion questions:
        What is the difference between “free as in free beer” and “free as in freedom”?
        Have you seen “open” being used for something not fulfilling the Open Definition?
    </small>
</section>


<section>
<h3>Four Freedoms in the OSI Open Source AI Definition</h3>

<small>
    An Open Source AI is an AI system made available under terms and in a way that grant the freedoms to:
</small>
<ul>
    <li><b>Use</b> the system for any purpose and without having to ask for permission.</li>
    <li><b>Study</b> how the system works and inspect its components.</li>
    <li><b>Modify</b> the system for any purpose, including to change its output.</li>
    <li><b>Share</b> the system for others to use with or without modifications, for any purpose.</li>
</ul>
<small>
    These freedoms apply both to a fully functional system and to discrete elements of a system.
    A precondition to exercising these freedoms is to have access to the preferred form to make modifications to the system.
</small>

<small>
    <em>Open Source Initiative</em> maintains the <em>Open Source Definition</em> and the <em>Open Source AI Definition</em>.
</small>
<br>

<img src="img/logos/open_source.png" class="stretch">
</section>


<section>
    <h3>Four Freedoms in the GNU Philosophy</h3>

    <small>
        A program is free software if the program's users have the four essential freedoms:
    </small>

    <ul>
        <li>The freedom to <b>run</b> the program as you wish, for any purpose (freedom 0).</li>
        <li>
            The freedom to <b>study</b> how the program works, and <b>change</b> it so it does your computing as you wish (freedom 1).
            <small>
                Access to the source code is a precondition for this.
            </small>
        </li>
        <li>The freedom to <b>redistribute</b> copies so you can help others (freedom 2).</li>
        <li>
            The freedom to <b>distribute</b> copies of your modified versions to others (freedom 3).
            <small>
                By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this.
            </small>
        </li>
    </ul>

    <small>
        What is Free Software? by <em>GNU</em> Operating System
        supported by the <em>Free Software Foundation</em>,
        generalized in <em>Free Cultural Works</em> (<a href="https://freedomdefined.org/">freedomdefined.org</a>)
    </small>

<br>

<img src="img/logos/gnu_logo.svg" class="stretch">

</section>


<section>
    <h3>Licensing</h3>

    <ul class="left">
        <li>Creative Commons (CC) licenses
        <li>GNU licenses, BSD licenses, ...
            <ul>
                <li>e.g., GNU GPL 3.0
            </ul>
        <li>Choosing a license and lists of licenses:
            <ul>
                <li><a href="https://choosealicense.com/">choosealicense.com</a> <small>(for software)</small>
                <li><a href="https://creativecommons.org/choose/">creativecommons.org/choose</a> <small>(CC licenses; for anything except software)</small>
                <li><a href="http://ufal.github.io/public-license-selector/">License Selector</a> <small>(software and CC licenses)</small>
                <li><a href="https://www.openaire.eu/research-data-how-to-license/">OpenAIRE</a> <small>(data license Q&amp;A)</small>
                <li><a href="https://opendatacommons.org/">Open Data Commons</a>
                <li><a href="https://opensource.org/licenses">Open Source Initiative</a>
                <li><a href="https://www.gnu.org/licenses/license-list.html">GNU (Free Software Foundation)</a>
            </ul>
    </ul>

    <div class="right">
    <img style="max-height: 15em !important;" src="img/cc_license_spectrum.png">
    <p class="credit">
        “Creative Commons License Spectrum” by <a href="https://commons.wikimedia.org/wiki/File:Creative_commons_license_spectrum.svg">Shaddim</a> (CC BY 4.0),
        <a href="https://creativecommons.org/share-your-work/public-domain/freeworks">Creative Commons: Understanding Free Cultural Works</a>
    </p>
    </div>

    <p>
    <small class="glow">
        Discussion questions:
        Do you read “terms and conditions”?
        Have you ever read any “terms and conditions” or end user license agreement (EULA)?
        What about an open source software license?
    </small>

</section>


<section>
<h3>Open Science Components</h3>

<div class="fragment">

<ul class="left">
<li>6 pillars <small>[Watson 2015]</small>:
    <ul>
        <li>open methodology
        <li>open access
        <li>open data
        <li>open source (software)
        <li>open peer review
        <li>open education (or educational resources)
    </ul>
<li>other components:
    <ul>
        <li>open hardware
        <li>open formats
        <li>open standards
        <li>open source AI
    </ul>
</ul>

<ul class="right">
<li>related concepts:
    <ul>
        <li>Open-notebook science
        <li>Provenance
        <li>FAIR, CARE, &hellip;
        <li>Science 2.0 <small>(like Web 2.0)</small>
        <li>Team science
        <li>Citizen science
        <li>Public science
        <li>Participatory research
        <li>Open innovation
        <li>Open organization
        <li>Crowdsourcing
        <li>Preprints
        <li>Inner source
    </ul>
</ul>

<p>
<small>
    Discussion questions:
    What would add to the list?
    What do you see something for the first time?
    What is openwashing?
</small>

</div>

</section>

<!--
Watson, M. (2015). When will ‘open science’ become simply ‘science’?.
Genome biology, 16(1), 101. doi:10.1186/s13059-015-0669-2
There are six commonly accepted pillars of open science: open data,
open access, open methodology, open source, open peer review and open education.
-->


<section>
    <h3>FAIR Principles</h3>

    <ul>
        <li>Findable <small>persistent identifier and metadata for data</small>
        <li>Accessible <small>sharing protocol is open, free, and universally implementable</small>
        <li>Interoperable <small>formal language and references to other datasets</small>
        <li>Reusable <small>clear usage license (can be restricted), detailed provenance</small>
    </ul>

    <p>
        <em>
            The principles emphasize machine-actionability (&hellip;)
            because humans increasingly rely on computational support to deal with data (&hellip;)
        </em>
    </p>

    <small class="credit">
        [Wilkinson 2016]
        <br>
        <a href="https://www.go-fair.org/fair-principles/">https://www.go-fair.org/fair-principles</a>
    </small>

    <p>
        Reactions: FAIREr (Explorability), FAIRER (Equitable and Realistic or Revisible), CARE (Collective Benefit, Authority to Control, Responsibility, Ethics), Good Data
    </p>

    <br>
    <img class="stretch" src="img/fair_data_principles.jpg">
    <p class="credit">
        <small>
            Image: “FAIR guiding principles for data resources” by <a href="https://commons.wikimedia.org/wiki/File:FAIR_data_principles.jpg">SangyaPundir</a> (CC BY-SA 4.0)
        </small>
    </p>

    <p class="glow">
    <small>
        Discussion questions:
        Which parts are unique to FAIR and not present in open science?
        Is source code part of data, data provenance, or it is a separate thing?
    </small>
</section>


<section>
    <h3>The “re” Words</h3>

    <small>
    No agreement on some of the definitions especially in different fields;
    definitions are often overlapping or swapped, some don't make any distinction.
    </small>

    <ul>
        <li>replicability <small>independent validation of specific findings</small>
        <li>repeatability <small>same conditions, people, instruments, ... (test–retest reliability)</small>
        <li>reproducibility <small>same results using the same raw data or same materials, procedures, ...</small>
        <li>recomputability <small>same results received by computation (in computational research)</small>
        <li>reusability <small>use again the same data, tools, or methods</small>
    </ul>

    <small>
    For example, Ince et al. (2012) in computational science
    distinguishes <em>direct reproducibility</em> as rerunning the code and
    <em>indirect reproducibility</em> as validating something other than the entire code.
    </small>

    <br>
    <img class="stretch" src="img/recycle.png">

    <!--
        Openclipart, Public Domain, Green recycling, garethclubb
        https://openclipart.org/detail/171146/green-recycling
    -->

    <br>
    <small class="glow">
        Discussion questions:
        As a PhD student, which of these features would you like to see in other research?
        As a journal paper reviewer, what should you be able to do
        when you receive a scientific publication for review?
    </small>

    </section>


<section>
<h3>Computational and Geospatial Research</h3>

<ul>
    <li>
        code is a part of method description
        <small>[Ince et al. 2012, Morin et al. 2012, Nature Methods 2007]</small>
    <li>
        use of open source tools is a part of reproducibility
        <small>[Lees 2012, Alsberg &amp; Hagen 2006]</small>
    <li>
        <em>easily reproducible result</em> is a result obtained in 10 minutes
        <small>[Schwab et al. 2000]</small>
    <li>
        geospatial research specifics:
        <ul>
            <li>some research introduces new code
            <li>some research requires significant dependencies
            <li>some research produces user-ready software
        </ul>
</ul>

<p>
<small>
    Discussion questions:
    Is spatial special?
    Is recomputing the results useful for research?
    How long should it take to recompute results?
    Do dependencies need to be open source as well?
</small>

</section>


<section>
    <h3>Open Science Publication: Use Case</h3>
    Petras et al. 2017
    <br>
    <img class="stretch" src="img/petras2017generalized_full.png">
    <br>
    <small class="credit">
        Petras, V., Newcomb, D. J., &amp; Mitasova, H. (2017).
        <em>Generalized 3D fragmentation index derived from lidar point clouds.</em>
        In: Open Geospatial Data, Software and Standards 2(1), 9.
        <a href="https://doi.org/10.1186/s40965-017-0021-8">DOI 10.1186/s40965-017-0021-8</a>
    </small>
</section>


<section>
<h3>Open Science Publication: Components</h3>
<table style="font-size: smaller;">
    <tr>
        <th>Publication Component</th>
        <th>in the Petras et al. 2017 use case</th>
    <tr>
        <td>Text</td>
        <td>
            background, methods, results, discussion, conclusions, &hellip; <small>(OA)</small>
        </td>
    </tr>
    <tr>
        <td>Data</td>
        <td>
            input data <small>(formats readable by open source software)</small>
        </td>
    </tr>
    <tr>
        <td>Reusable&nbsp;Code</td>
        <td>
            methods as GRASS tools <small>(C &amp; Python)</small>
        </td>
    </tr>
    <tr>
        <td>Publication-specific&nbsp;Code</td>
        <td>
            scripts to generate results <small>(Bash &amp Python)</small>
        </td>
    </tr>
    <tr>
        <td>Computational Environment</td>
        <td>
            details about all dependencies and the code <small>(Docker, Dockerfile*)</small>
        </td>
    </tr>
    <tr>
        <td>Versions</td>
        <td>
            repository with current and previous versions* <small>(Git, GitHub)</small>
        </td>
    </tr>
</table>
<p>
    <small style="font-size: 70%;">
    * Version associated with the publication included also as a supplemental file.
    </small>
</p>

<small class="credit">
Petras, V. (2018). <em>Geospatial analytics for point clouds
in an open science framework</em>. Doctoral dissertation.
<a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">URI&nbsp;http://www.lib.ncsu.edu/resolver/1840.20/35242</a>
</small>

<p>
<small>
    Discussion questions:
    What are other technologies which are good fit for these components?
    Are there other components or categories?
    What parts of research did you publish or tried to publish and what challenges did you face?
</small>

</section>


<section>
    <h3>Open Science Publication: In a Single Package Online</h3>
    <ul>
        <li>Components other than Text and Versions for Petras et al. 2017 are now also available at <a href="https://doi.org/10.24433/CO.3986355.v2" title="Goes to the capsule">Code Ocean</a> as a capsule.
    </ul>

    <img class="stretch" style="margin: 0px"; src="img/code_ocean_petras_2017.png" alt="Code Ocean capsule content in a web browser">
    <br>
    <small class="credit">
    <a href="https://doi.org/10.24433/CO.3986355.v2">DOI&nbsp;10.24433/CO.3986355.v2</a>
    </small>

    <p class="glow">
    <small>
        Discussion questions:
        What is the skill set needed to publish results like this?
        What is the long-term sustainability of online recomputability tools such as Code Ocean?
    </small>
</section>


<section>
    <h3>Open Science Publication: Software Platform</h3>
    <ul>
        <li>Preprocessing, visualization, and interfaces <small>(GUI, CLI, API)</small>
        <li>Data inputs and outputs, memory management
        <li>Integration with existing analytical tools
        <li>Preservation of the reusable code component <small>(long-term maintenance)</small>
        <li>Dependency which would be hard to change for something else
        <li>Example: FUTURES model implemented as a set of GRASS tools
            <small>(<em>r.futures.pga, r.futures.demand, r.futures.parallelpga, ...</em>)</small>
    </ul>

    <small class="credit">
        Petras, V. (2018). <em>Geospatial analytics for point clouds
        in an open science framework</em>. Doctoral dissertation.
        <a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">URI&nbsp;http://www.lib.ncsu.edu/resolver/1840.20/35242</a>
    </small>

    <p class="glow">
    <small>
        Discussion questions:
        What software can play this role?
        What are the different levels of integration with a piece of software and their advantages and disadvantages?
    </small>
</section>


<section>
    <h2>Challenges</h2>
</section>


<section>
    <h3>Focus on Novelty in Publishing and Funding</h3>

    <blockquote>
    It would be preferable to have the time to do it right, while simultaneously allowing scientists to be human and make mistakes, instead of focusing on novelty, being first and publishing in highly selective journals.
        <small>
            <br>
            Holding, A. N. (2019). Novelty in science should not come at the cost of reproducibility. The FEBS Journal, 286(20), 3975–3979. <a href="https://doi.org/10.1111/febs.14965">DOI 10.1111/febs.14965</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What research gets published?
        What research gets funded?
    </small>
</section>


<section>
    <h3>Lack of Incentives for Reproducibility</h3>

    <blockquote>
        [...] irreproducible research [...] careers [...] personal cost:
        young scientists [...] their families [...] visas that are conditional [...]
        Running out of time
        [...]
        pressure on early-career researchers to deliver high-impact results.
        The outcome is an environment that pushes people to get across the line as quickly as possible, while the incentives to challenge or to reproduce previous studies are minimal.
        <small>
            <br>
            Holding, A. N. (2019). Novelty in science should not come at the cost of reproducibility. The FEBS Journal, 286(20), 3975–3979. <a href="https://doi.org/10.1111/febs.14965">DOI 10.1111/febs.14965</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        How important is to challenge or reproduce previous studies?
        How important is being able to reproduce your own studies?
    </small>
</section>


<section>
    <h3>Scooping</h3>

    <blockquote>
        ‘There is always this fear, that someone steals your ideas, or is doing the same thing at the same
        time, and some people fear it more than other people, I think especially younger people, also some
        older. I think this causes a lot of stress to the scientists, and it has happened to me. […] you try not to
        think about it, you still think that what if someone else is doing the same thing and this is useless
        work, so then it takes your energy.’
        <small>
            <br>
            &mdash; research participant in
            <br>
            Laine, Heidi (2017). Afraid of scooping: Case study on researcher strategies against fear of scooping in the context of open science. Data Science Journal.
            <a href="https://doi.org/10.5334/dsj-2017-029">DOI 10.5334/dsj-2017-029</a>
        </small>
    </blockquote>
    <small class="glow">
        Discussion questions:
        What is scooping (being scooped) in science?
        Are you afraid of it?
        <br>
        PLOS publishes
        <a href="https://theplosblog.plos.org/2020/04/the-importance-of-being-second-plos-wide-edition/">scooped research</a>
        and
        <a href="https://journals.plos.org/plosone/s/what-we-publish">negative and null results</a>
    </small>
</section>


<section>
    <h3>Content Creation and GenAI</h3>
    <blockquote>
        Well-constructed AI systems generally do not regenerate, in any nontrivial portion, unaltered data from any particular work in their training corpus.
        <small>
            <br>
            &mdash; OpenAI, LP.
            Before the United States Patent and Trademark Office, Department of Commerce.
            <a href="https://cdn.openai.com/policy-submissions/OpenAI%20Comments%20on%20Intellectual%20Property%20Protection%20for%20Artificial%20Intelligence%20Innovation.pdf">
                Comment Regarding Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation.
                Docket No. PTO–C–2019–0038
            </a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What have you seen an LLM or other GenAI generate?
        Was there a copyright or IP issue?
    </small>
</section>


<section>
    <h3>Copyright, Licensing, and GenAI</h3>
    <blockquote>
        &hellip;GitHub Copilot trains on non permissive licenses&hellip;
        It should be worrisome how easily GitHub Copilot spits out GPL code
        without being prompted adversarially,
        independent of what they and other researchers claim.
        <small>
            <br>
            &mdash;
            <a href="https://windsurf.com/blog/copilot-trains-on-gpl-codeium-does-not">GitHub Copilot Emits GPL. Codeium Does Not.</a>
            Windsurf Team. Apr 20, 2023. Accessed 2025-10-24.
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Are LLMs/GenAIs different from text and data mining tools?
        Is copyright obsolete or should it be more enforced?
    </small>
</section>


<section>
    <h3>GenAI and Open Source Infrastructure</h3>
    <blockquote>
        Once AI training sets subsume the collective work of decades of open collaboration,
        the global commons idea, substantiated into repos and code all over the world,
        risks becoming a nonrenewable resource, mined and never replenished.
        <small>
            &hellip;
            What makes this moment especially tragic is that the very infrastructure enabling generative AI was born from the commons it now consumes.
            Free and open source software built the Internet&hellip;
            Every cloud provider, every hyperscale data center, every LLM pipeline sits on a foundation of FOSS.
        </small>
        <small>
            <br>
            &mdash; Sean O'Brien, founder of the Yale Privacy Lab at Yale Law School in
            <a href="https://www.zdnet.com/article/why-open-source-may-not-survive-the-rise-of-generative-ai/">Why open source may not survive the rise of generative AI</a>
            by David Gewirtz. ZDNET. Oct. 24, 2025. Accessed 2025-10-27.
        </small>
    </blockquote>
</section>


<section>
    <h3>Private Data</h3>

    <blockquote>
        [...] releasing datasets as open data may threaten privacy, for instance if
        they contain personal or re-identifiable data. Potential privacy problems
        include chilling effects on people communicating with the public sector, a
        lack of individual control over personal information, and discriminatory
        practices enabled by the released data.
        <small>
            <br>
            Borgesius, F. Z., Gray, J., & van Eechoud, M. (2016). Open Data, Privacy, and Fair Information Principles: Towards a Balancing Framework. <a href="https://doi.org/10.15779/Z389S18">10.15779/Z389S18</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you use or create personal or private data in your research or do you expect you will?
    </small>
</section>


<section>
    <h3>Sensitive Data</h3>

    <blockquote>
        Insights obtained by compiling public information from Open Data sources,
        may represent a risk to Critical Infrastructure Protection efforts. This knowledge
        can be obtained at any time and can be used to develop strategic plans
        of sabotage or even terrorism.
    <small>
    <br>
    Fontana, R. (2014). Open Data analysis to retrieve sensitive information regarding national-centric critical infrastructures.
    <a href="http://open.nlnetlabs.nl/downloads/publications/RP45%20Open%20Data%20Analysis%20-%20Critical%20infrastructures.pdf">http://open.nlnetlabs.nl/downloads/publications/...</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you use or create sensitive data in your research or do you expect you will?
    </small>
</section>


<section>
    <h3>Publishing Source Code with a Paper</h3>

    <blockquote>
        [...] that’s going to be harder.
        [...] I’m expecting to get screenshots of MATLAB procedures and horrible Python code that even the author can’t read anymore, and I don’t know what we’re going to do about that.
        Because in some sense, you can’t push too hard because if they go back and rewrite the code or clean it up, then they might actually change it.
        <small>
            <br>
            &mdash; An interviewed journal editor-in-chief in
            <br>
            Sholler, D., Ram, K., Boettiger, C., &amp; Katz, D. S. (2019). Enforcing public data archiving policies in academic publishing: A study of ecology journals. Big Data &amp; Society, 6(1).
            <a href="https://doi.org/10.1177/2053951719836258">DOI 10.1177/2053951719836258</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Have you ever broadly shared source code or other internal parts of your work?
    </small>
</section>


<section>
    <h3>Open Source Software and Research Funding</h3>

    <!-- focus on new research, not maintenance (novelty not sustainability) -->

    <blockquote>
        “That’s really the tragedy of the funding agencies in general,” says Carpenter. “They’ll fund 50 different groups to make 50 different algorithms, but they won’t pay for one software engineer.”
        <small>
            <br>
            &mdash; Anne Carpenter, a computational biologist at the Broad Institute of Harvard and MIT in Cambridge in
            <br>
            Nowogrodzki, Anna (2019). How to support open-source software and stay sane. Nature, 571(7763), 133–134. <a href="https://doi.org/10.1038/d41586-019-02046-0">DOI 10.1038/d41586-019-02046-0</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What open source software which high-relevant to research do you know?
        Any idea about how it is funded?
    </small>
</section>


<section>
    <h3>Open Source Software and Government</h3>

    <blockquote>
        <p>
            [around 1990] [...] GIS industry claimed that it was unfair for the Federal Government to be competing with them.
            <small>
                <br>
                Westervelt, J. (2004). GRASS Roots. <a href="https://grass.osgeo.org/files/westervelt2004_GRASS_roots.pdf">Proceedings of the FOSS/GRASS Users Conference</a>. Bangkok, Thailand.
            </small>
        </p>
        <p>
            In 1996 USA/CERL, [...] announced that it was formally withdrawing support [...and...] announced agreements with several commercial GISs, and agreed to provide encouragement to commercialization of GRASS. [...] result is a migration of several former GRASS users to COTS
            [...]
            The first two agreements encouraged the incorporation of GRASS concepts into ESRI's and Intergraph's commercial GISs.
            <small>
                <br>
                Hastings, D. A. (1997). The Geographic Information Systems: GRASS HOWTO. <a href="https://tldp.org/HOWTO/GIS-GRASS/index.html">tldp.org/HOWTO/GIS-GRASS</a>
                <br>
                Original announcement: <a href="https://grass.osgeo.org/news/cerl1996/grass.html">grass.osgeo.org/news/cerl1996/grass.html</a>
            </small>
        </p>

        <!--
            Corps Lab Ends GRASS Development
            https://grass.osgeo.org/news/cerl1996/grass.html
            While GRASS is public domain software, several companies use it or GRASS-like features in their commercial, off-the-shelf (COTS) products.
            These features provide the tools that had been identified as critical for military use.
            CERL is entering into partnering agreements with several key companies to help ensure continued support to military GIS users.
            To date, an agreement has been drafted with the Environmental Systems Research Institute, Inc., (ESRI), which produced Arc/Info, Arc View, and other products.
            CERL is also seeking agreements with Intergraph, MGE’s producer, and with Logiciels et Applications Scientifiques, Inc. (LAS),
            a Montreal company providing COTS software products (GRASSLANDS) running in a PC environment and based on GRASS.

            Under the planned agreements, CERL will take an active role in helping the industry partner understand the GIS needs at military installations
            and within the Army Corps of Engineers.
            In addition, CERL will continue developing advanced technology for geospatial modeling and specific applications related to land management.
            This will be cooperative development with COTS vendors to ensure all products will work with the systems used at military sites.
        -->
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you know GRASS?
    </small>
</section>


<!--
<section>
    <h3>NSF Open-Source Ecosystem Grant</h3>
    <ul>
        <li>$1.5M NSF grant awarded to <b>NC State</b>, ASU, NMSU, Yale.</li>
        <li>To enhance infrastructure, revise contributing guidelines and to support community building of GRASS GIS.</li>
        <li>The NSF program is aiming at improving sustainability, not sustaining the project, adding features, or fixing bugs.</li>
    </ul>
    <img src="img/nsf_grant_announcement.png" class="stretch">
    <small class="glow">
        Discussion questions:
        Anything surprising in what is funded? Why? Why not?
    </small>
</section>


<section>
    <h2>Turn of the Tide?</h2>
</section>


<section>
    <h3>National Institutes of Health New 2023 Policy</h3>

    <blockquote>
    NIH expects that [...] researchers will maximize the appropriate sharing of scientific data, acknowledging certain factors (i.e., legal, ethical, or technical) [...]
    Shared scientific data should be made accessible as soon as possible [...]
    <small>
    <br>
    NOT-OD-21-013: Final NIH Policy for Data Management and Sharing. Retrieved November 6, 2024, from <a href="https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html">https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Would you expect a health-related organization to be on the forefront of sharing data?
    </small>
</section>


<section>
    <h3>2023: Year of Open Science</h3>
    <blockquote>
        [The White House Office of Science and Technology Policy (OSTP)] is [...]
        launching the Year of Open Science, featuring actions across the federal
        government throughout 2023 to advance national open science policy,
        provide access to the results of the nation’s taxpayer-supported research,
        accelerate discovery and innovation, promote public trust,
        and drive more equitable outcomes.
        <small>
            <br>
            FACT SHEET: Biden-Harris Administration Announces New Actions to Advance Open and Equitable Research.
            January 11, 2023.
            <a href="https://www.whitehouse.gov/ostp/news-updates/2023/01/11/fact-sheet-biden-harris-administration-announces-new-actions-to-advance-open-and-equitable-research/">whitehouse.gov/...open-and-equitable-research</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Anything you consider new?
    </small>
</section>


<section>
    <h3>NASA's 2022 commitment</h3>
    <blockquote>
        [In 2022,] NASA committed $20 million per year to advance open science, beginning in 2023.
        <small>
            <br>
            Why NASA and federal agencies are declaring this the Year of Open Science.
            Nature 613, 217 (2023).
            <a href="https://doi.org/10.1038/d41586-023-00019-y">DOI 10.1038/d41586-023-00019-y</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What do you think this will be spend on?
    </small>
</section>


<section>
    <h3>Nelson Memo: Peer-Reviewed Publications (2025)</h3>

    <blockquote>
        [...] all peer-reviewed scholarly publications [...] resulting from federally funded research are made freely
        available [...] without any [...] delay after publication.
    <small>
    <br>
    White House Office of Science and Technology Policy (2022). Desirable Characteristics of Data Repositories for Federally Funded Research. <a href="https://doi.org/10.5479/10088/113528">DOI 10.5479/10088/113528</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What open-science concept this refers to?
    </small>
</section>


<section>
    <h3>Nelson Memo: Scientific Data (2025)</h3>

    <blockquote>
        Scientific data underlying peer-reviewed scholarly publications resulting from
        federally funded research should be made freely available [...] at the time of publication, unless subject to limitations [...]
    <small>
    <br>
    White House Office of Science and Technology Policy (2022). Desirable Characteristics of Data Repositories for Federally Funded Research. <a href="https://doi.org/10.5479/10088/113528">DOI 10.5479/10088/113528</a>

    <small>
        [...] “scientific data” include the recorded factual material commonly accepted
        in the scientific community as of sufficient quality to validate and replicate research findings. Such scientific data do
        not include laboratory notebooks, preliminary analyses, case report forms, drafts of scientific papers, plans for future
        research, peer-reviews, communications with colleagues, or physical objects and materials, such as laboratory
        specimens, artifacts, or field notes.
    </small>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What open-science concept or concepts this refers to?
    </small>
</section>


<section>
    <h3>Open Source Software and Industry</h3>

    <blockquote>
        Open source became a movement – a mentality. Suddenly infrastructure
        software was nearly free [comparing to 1999]. We paid 10% of the normal
        costs for the software and that money was for software support. A
        90% disruption in cost spawns innovation – believe me.
    <small>
        <br>
        &mdash; Mark Suster (2011) in
        <br>
        Eghbal, Nadia (2016). Roads and bridges: The unseen labor behind our digital infrastructure. <a href="https://www.fordfoundation.org/work/learning/research-reports/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure/">Ford Foundation</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you know any open-source software success stories?
    </small>
</section>
-->


<section>
    <h2>Motivation</h2>
</section>


<!--
<section>
<h3>Open Science Beginnings</h3>

<p>
First journal ever published:<br>
<em>Philosophical Transactions (of the Royal Society)</em>
</p>

<img src="img/philosophical_transactions.jpg" class="stretch">
<p class="credit glow">
    CC BY Stefan Janusz,
    <a href="https://en.wikipedia.org/wiki/File:Philosophical_Transactions_Volume_1_frontispiece.jpg">Wikipedia</a>
</p>
</section>
-->

<section>
<h3>Theoretical Publishing Goals</h3>
<ul>
    <li>registration <small>so that scientists get credit</small>
    <li>archiving <small>so that we preserve knowledge for the future</small>
    <li>dissemination <small>so that people can use this knowledge</small>
    <li>peer review <small>so that we know it's worth it</small>
</ul>

<p>
<small>
    Discussion questions:
    <!-- How are these goals different from goals of science? -->
    How are these publishing goals fulfilled by journal papers?
</small>
</section>


<section>
    <h3>Internal Reasons for Open Science</h3>

    <ul>
        <li>Open science in your lab (team-oriented reasons):
            <ul>
                <li>collaboration <small>work together with your colleagues</small>
                <li>transfer <small>transfer research between you and your colleague</small>
                <li>longevity <small>re-usability of parts of research over time</small>
            </ul>
        <li>Open science by yourself (“selfish” reasons):
            <ul>
                <li>revisit <small>revisit or return to a project after some time</small>
                <li>correction <small>correct a mistake in the research</small>
                <li>extension <small>improve or build on an existing project</small>
            </ul>
    </ul>

    <p>
    <small>
        Discussion questions:
        What is your experience with getting back to your own research
        or continuing research started by someone else?
        <small>(See <a href="http://phdcomics.com/comics.php?f=1689">PhD Comics: Scratch</a>.)</small>
        How does open science relate to team science?
        How making things public can help us to achieve the desired effect and what challenges that brings?
    </small>

</section>


<!--
<section>
<h3>References</h3>

<small style="font-size: 50%" class="glow">

<ul>
<li>
Alsberg, Bjørn K., and Ole Jacob Hagen. “How Octave Can Replace Matlab in Chemometrics.” Chemometrics and Intelligent Laboratory Systems, Selected papers presented at the 9th Scandinavian Symposium on Chemometrics Reykjavik, Iceland 21–25 August 2005, 84, no. 1 (December 1, 2006): 195–200. <a href="https://doi.org/10.1016/j.chemolab.2006.04.025">doi:10.1016/j.chemolab.2006.04.025</a>.
<li>
Buckheit, Jonathan B., and David L. Donoho. “WaveLab and Reproducible Research.” In Wavelets and Statistics, edited by Anestis Antoniadis and Georges Oppenheim, 103:55–81. Lecture Notes in Statistics. New York, NY: Springer New York, 1995. <a href="https://doi.org/10.1007/978-1-4612-2544-7_5">doi:10.1007/978-1-4612-2544-7_5</a>.
<li>
Ince, Darrel C., Leslie Hatton, and John Graham-Cumming. “The case for open computer programs”. In: Nature 482.7386 (2012), pp. 485–488. <a href="http://doi.org/10.1038/nature10836">doi:10.1038/nature10836</a>
<li>
Lees, Jonathan M. “Open and free: Software and scientific reproducibility”. In: Seismological Research Letters 83.5 (2012), pp. 751–752. <a href="http://doi.org/10.1785/0220120091">doi:10.1007/s10816-015-9272-9</a>
<li>
Marwick, Ben. “Computational reproducibility in archaeological research: basic principles and a case study of their implementation”. In: Journal of Archaeological Method and Theory 24.2 (2017), pp. 424–450. <a href="http://doi.org/10.1007/s10816-015-9272-9">doi:10.1007/s10816-015-9272-9</a>
<li>
Morin, A et al. “Shining light into black boxes”. In: Science 336.6078 (2012), pp. 159–160. <a href="http://doi.org/10.1126/science.1218263">doi:10.1126/science.1218263</a>
<li>
Nature Publishing Group. “Social Software.” Nature Methods 4, no. 3 (March 2007): 189. <a href="https://doi.org/10.1038/nmeth0307-189">doi:10.1038/nmeth0307-189</a>.
<li>
Peng, Roger D. “Reproducible Research in Computational Science.” Science (New York, N.Y.) 334, no. 6060 (December 2, 2011): 1226–27. <a href="https://doi.org/10.1126/science.1213847">doi:10.1126/science.1213847</a>
<li>
Petras, Vaclav. “Geospatial analytics for point clouds in an open science framework.” Doctoral dissertation. 2018. <a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">www.lib.ncsu.edu/resolver/1840.20/35242</a>
<li>
Petras, Vaclav, Douglas J. Newcomb, and Helena Mitasova. “Generalized 3D Fragmentation Index Derived from Lidar Point Clouds.” Open Geospatial Data, Software and Standards 2, no. 1 (April 2017): 9. <a href="https://doi.org/10.1186/s40965-017-0021-8">doi:10.1186/s40965-017-0021-8</a>.
<li>
Rocchini, Duccio and Markus Neteler. “Let the four freedoms paradigm apply to ecology”. In: Trends in Ecology and Evolution (2012). <a href="http://doi.org/10.1016/j.tree.2012.03.009">doi:10.1016/j.tree.2012.03.009</a>
<li>
Rodriguez-Sanchez, Francisco, Antonio Jesús Pérez-Luque, Ignasi Bartomeus, and Sara Varela. “Ciencia Reproducible: Qué, Por Qué, Cómo.” Revista Ecosistemas 25, no. 2 (2016): 83–92.
<li>
Schwab, Matthias, Martin Karrenbach, and Jon Claerbout. “Making Scientific Computations Reproducible.” Computing in Science & Engineering 2, no. 6 (2000): 61–67. <a href="https://doi.org/10.1109/5992.881708">doi:10.1109/5992.881708</a>.
<li>
Stodden, V., Seiler, J., &amp; Ma, Z. (2018). “An empirical analysis of journal policy effectiveness for computational reproducibility.” In: <em>Proceedings of the National Academy of Sciences</em> 115(11), p. 2584-2589. <a href="https://doi.org/10.1073/pnas.1708290115">doi:10.1073/pnas.1708290115</a>
<li>
Watson, M. (2015). When will ‘open science’ become simply ‘science’?. Genome biology, 16(1), 101. <a href="https://doi.org/10.1186/s13059-015-0669-2">doi:10.1186/s13059-015-0669-2</a>
<li>
Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., ... &amp; Bouwman, J. (2016).
“The FAIR Guiding Principles for scientific data management and stewardship”. <em>Scientific data</em>, 3(1), 1-9.
<a href="https://doi.org/10.1038/sdata.2016.18">doi:10.1038/sdata.2016.18</a>
</ul>

</small>

</section>
-->


<section>
    <h3>Resolution for Class Debate</h3>

    <blockquote>
        A scientific publication needs to consist of text, data, source code, computational software environment, and reviews
        which are all openly licensed, in open formats, checked during the submission process, and
        publicly available without any delay at the time of publication.
    </blockquote>

    <small class="glow">
        Consider:
        Funding, workload, scooping, AI/GenAI/LLMs, tax payers, industry, missions of funding agencies, &hellip;
    </small>
</section>
<!-- This is a generated file. Do not edit. -->
        </div>  <!-- slides -->

    </div>  <!-- reveal -->

    <!--<img class="timeline" id="line" />-->

    <!--
        Home button or link to a parent page
        If you want this to be unique for every page (slide deck),
        then remove it from here and put it at the end of each
        file (or series of files) creating one page
        (the position will be little different)
        TODO: some JS is needed to move it to the right position
    -->
    <div class="parent-page">
        <!-- alternative symbol: &#x1f3e0; -->
        <a href=".." title="Course website">
            <img width="15px" src="img/home.svg"></a>
    </div>

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.js"></script>

        <script>

            // Full list of configuration options available here:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                // Display controls in the bottom right corner
                controls: false,

                // Display a presentation progress bar
                progress: true,
                
                center: true,
                
                // Display the page number of the current slide
                slideNumber: false,

                // Enable the slide overview mode
                overview: true,

                // Turns fragments on and off globally
                fragments: true,

                // The "normal" size of the presentation, aspect ratio will be preserved
                // when the presentation is scaled to fit different resolutions. Can be
                // specified using percentage units.
                // width: 960,
                // height: 700,
                
                // Factor of the display size that should remain empty around the content
                margin: 0.05,  // increase?

                // Bounds for smallest/largest possible scale to apply to content
                minScale: 0.5,
                maxScale: 5.0,

                theme: Reveal.getQueryHash().theme,  // available themes are in /css/theme
                transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/fade/none

                // Push each slide change to the browser history
                history: true,
                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,
                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'none', // default/none/slide/concave/convex/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"
                chalkboard: { 
        		// optionally load pre-recorded chalkboard drawing from file
            		src: "chalkboard.json",
            	},
                // Optional libraries used to extend on reveal.js
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
                    /* { src: 'plugin/math/math.js', async: true }, */
                    { src: 'plugin/chalkboard/chalkboard.js' }
                ],
                keyboard: {
            	    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle notes canvas when 'c' is pressed
            	    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
            	    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
            	     8: function() { RevealChalkboard.reset() },	// reset chalkboard data on current slide when 'BACKSPACE' is pressed
            	    68: function() { RevealChalkboard.download() },	// downlad recorded chalkboard drawing when 'd' is pressed
            	},
            });

        </script>

    </body>
</html>
