<section>
<h2>Open Science</h2>
<h3 style="margin-top: 0.5em">
    Vaclav (Vashek) Petras</h3>
<p class="title-foot">
    <a href="http://geospatial.ncsu.edu/osgeorel/">NCSU GeoForAll Lab</a>
    at the
    <a href="http://geospatial.ncsu.edu/">Center for Geospatial Analytics</a>
    <br>
    <a href="http://www.ncsu.edu/">North Carolina State University</a>
</p>
<img style="width: 25%;" src="img/logos/ncstate.png">
<p>
    GIS 710: Geospatial Analytics for Grand Challenges<br>
    November 13, 2023
</p>
<p>
    <a href="../topics/open-science-for-grand-challenges.html" style="font-size: 150%;">https://bit.ly/2oR9owy</a>
</p>
</section>


<section>
    <h3>Learning Objectives</h3>
    <ul>
        <li>Understating motivation for practicing open science
        <li>Understating complexity of practicing open science
        <li>Critical thinking about pros, cons, and challenges
        <li>General understanding of tools and services involved
        <li>Practical knowledge of tools for sharing research and computations
        <li>Ideas about how to use them in complex geospatial applications
    </ul>
</section>


<section>
    <h3>Reproducibility of Computational Articles</h3>
    <p>
    Stodden et al. (PNAS, March 13, 2018)<br>
    204 computational articles from <em>Science</em> in 2011–2012
    </p>
    <img class="stretch" style="margin: 0px"; src="img/reproducibility_ratio_from_stodden.png" alt="26% reproducible, 74% irreproducible">
    <small class="credit">
    Stodden, V., Seiler, J., &amp; Ma, Z. (2018).
    An empirical analysis of journal policy effectiveness for computational reproducibility.
    In: <em>Proceedings of the National Academy of Sciences</em>
    115(11), p. 2584-2589.
    <a href="https://doi.org/10.1073/pnas.1708290115">DOI&nbsp;10.1073/pnas.1708290115</a>
    </small>

    <small class="glow">
        Discussion questions:
        Do you know about similar studies?
        What do they say?
    </small>

    <aside class="notes">
        A recently published study showed that 74% of computational
        articles published in Science was not computationally reproducible.
        This brings questions about validity of findings in these
        articles and also about the peer-review process.
        <!--
        Are these articles useful even to read when we don't know how
        the findings were obtained? ...let alone applying the findings.
        -->
    </aside>
</section>

<!--
Stodden et al. (2018):
We were able to obtain data and code from the authors of 89
articles in our sample of 204, giving an estimate for the artifact
recovery rate of 44% for articles published in Science shortly
after the policy change: (65 + 24/204) with a 95% bootstrap
confidence interval of [0.36, 0.50]. Of the 56 articles that were
then deemed potentially reproducible, we randomly chose 22
to attempt replication, and all but 1 of the 22 provided enough
information that we were able to reproduce their computational
findings (given sufficient resources and a willingness write some
code). We estimate 95% (21/22) of the articles deemed reproducible
by inspection are computationally reproducible, so for
the full sample, we estimate 26% will computationally reproduce
((56 ∗ (1 − 1/22))) with a 95% bootstrap confidence interval for
the proportion [0.20, 0.32].
((56 * (1 - 1./22)) / 204)
-->

<section>
    <h2>Challenges of Open Science</h2>
</section>


<section>
    <h3>Focus on Novelty in Publishing and Funding</h3>

    <blockquote>
    It would be preferable to have the time to do it right, while simultaneously allowing scientists to be human and make mistakes, instead of focusing on novelty, being first and publishing in highly selective journals.
        <small>
            <br>
            Holding, A. N. (2019). Novelty in science should not come at the cost of reproducibility. The FEBS Journal, 286(20), 3975–3979. <a href="https://doi.org/10.1111/febs.14965">DOI 10.1111/febs.14965</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What research gets published?
        What research gets funded?
    </small>
</section>


<section>
    <h3>Lack of Incentives for Reproducibility</h3>

    <blockquote>
        [...] irreproducible research [...] careers [...] personal cost:
        young scientists [...] their families [...] visas that are conditional [...]
        Running out of time
        [...]
        pressure on early-career researchers to deliver high-impact results.
        The outcome is an environment that pushes people to get across the line as quickly as possible, while the incentives to challenge or to reproduce previous studies are minimal.
        <small>
            <br>
            Holding, A. N. (2019). Novelty in science should not come at the cost of reproducibility. The FEBS Journal, 286(20), 3975–3979. <a href="https://doi.org/10.1111/febs.14965">DOI 10.1111/febs.14965</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        How important is to challenge or reproduce previous studies?
        How important is being able to reproduce your own studies?
    </small>
</section>


<section>
    <h3>Scooping</h3>

    <blockquote>
        ‘There is always this fear, that someone steals your ideas, or is doing the same thing at the same
        time, and some people fear it more than other people, I think especially younger people, also some
        older. I think this causes a lot of stress to the scientists, and it has happened to me. […] you try not to
        think about it, you still think that what if someone else is doing the same thing and this is useless
        work, so then it takes your energy.’
        <small>
            <br>
            &mdash; research participant in
            <br>
            Laine, Heidi (2017). Afraid of scooping: Case study on researcher strategies against fear of scooping in the context of open science. Data Science Journal.
            <a href="https://doi.org/10.5334/dsj-2017-029">DOI 10.5334/dsj-2017-029</a>
        </small>
    </blockquote>
    <small class="glow">
        Discussion questions:
        What is scooping (being scooped) in science?
        Are you afraid of it?
        <br>
        PLOS publishes
        <a href="https://theplosblog.plos.org/2020/04/the-importance-of-being-second-plos-wide-edition/">scooped research</a>
        and
        <a href="https://journals.plos.org/plosone/s/what-we-publish">negative and null results</a>
    </small>
</section>


<section>
    <h3>Private Data</h3>

    <blockquote>
        [...] releasing datasets as open data may threaten privacy, for instance if
        they contain personal or re-identifiable data. Potential privacy problems
        include chilling effects on people communicating with the public sector, a
        lack of individual control over personal information, and discriminatory
        practices enabled by the released data.
        <small>
            <br>
            Borgesius, F. Z., Gray, J., & van Eechoud, M. (2016). Open Data, Privacy, and Fair Information Principles: Towards a Balancing Framework. <a href="https://doi.org/10.15779/Z389S18">10.15779/Z389S18</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you use or create personal or private data in your research or do you expect you will?
    </small>
</section>


<section>
    <h3>Sensitive Data</h3>

    <blockquote>
        Insights obtained by compiling public information from Open Data sources,
        may represent a risk to Critical Infrastructure Protection efforts. This knowledge
        can be obtained at any time and can be used to develop strategic plans
        of sabotage or even terrorism.
    <small>
    <br>
    Fontana, R. (2014). Open Data analysis to retrieve sensitive information regarding national-centric critical infrastructures.
    <a href="http://open.nlnetlabs.nl/downloads/publications/RP45%20Open%20Data%20Analysis%20-%20Critical%20infrastructures.pdf">http://open.nlnetlabs.nl/downloads/publications/...</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you use or create sensitive data in your research or do you expect you will?
    </small>
</section>


<section>
    <h3>Publishing Source Code with a Paper</h3>

    <blockquote>
        [...] that’s going to be harder.
        [...] I’m expecting to get screenshots of MATLAB procedures and horrible Python code that even the author can’t read anymore, and I don’t know what we’re going to do about that.
        Because in some sense, you can’t push too hard because if they go back and rewrite the code or clean it up, then they might actually change it.
        <small>
            <br>
            &mdash; An interviewed journal editor-in-chief in
            <br>
            Sholler, D., Ram, K., Boettiger, C., &amp; Katz, D. S. (2019). Enforcing public data archiving policies in academic publishing: A study of ecology journals. Big Data &amp; Society, 6(1).
            <a href="https://doi.org/10.1177/2053951719836258">DOI 10.1177/2053951719836258</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Have you ever broadly shared source code or other internal parts of your work?
    </small>
</section>


<section>
    <h3>Open Source Software and Research Funding</h3>

    <!-- focus on new research, not maintenance (novelty not sustainability) -->

    <blockquote>
        “That’s really the tragedy of the funding agencies in general,” says Carpenter. “They’ll fund 50 different groups to make 50 different algorithms, but they won’t pay for one software engineer.”
        <small>
            <br>
            &mdash; Anne Carpenter, a computational biologist at the Broad Institute of Harvard and MIT in Cambridge in
            <br>
            Nowogrodzki, Anna (2019). How to support open-source software and stay sane. Nature, 571(7763), 133–134. <a href="https://doi.org/10.1038/d41586-019-02046-0">DOI 10.1038/d41586-019-02046-0</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What open source software which high-relevant to research do you know?
        Any idea about how it is funded?
    </small>
</section>


<section>
    <h3>Open Source Software and Government</h3>

    <blockquote>
        <p>
            [around 1990] [...] GIS industry claimed that it was unfair for the Federal Government to be competing with them.
            <small>
                <br>
                Westervelt, J. (2004). GRASS Roots. <a href="https://grass.osgeo.org/files/westervelt2004_GRASS_roots.pdf">Proceedings of the FOSS/GRASS Users Conference</a>. Bangkok, Thailand.
            </small>
        </p>
        <p>
            In 1996 USA/CERL, [...] announced that it was formally withdrawing support [...and...] announced agreements with several commercial GISs, and agreed to provide encouragement to commercialization of GRASS. [...] result is a migration of several former GRASS users to COTS
            [...]
            The first two agreements encouraged the incorporation of GRASS concepts into ESRI's and Intergraph's commercial GISs.
            <small>
                <br>
                Hastings, D. A. (1997). The Geographic Information Systems: GRASS HOWTO. <a href="https://tldp.org/HOWTO/GIS-GRASS/index.html">tldp.org/HOWTO/GIS-GRASS</a>
            </small>
        </p>

        <!--
            Corps Lab Ends GRASS Development
            https://grass.osgeo.org/news/cerl1996/grass.html
            While GRASS is public domain software, several companies use it or GRASS-like features in their commercial, off-the-shelf (COTS) products.
            These features provide the tools that had been identified as critical for military use.
            CERL is entering into partnering agreements with several key companies to help ensure continued support to military GIS users.
            To date, an agreement has been drafted with the Environmental Systems Research Institute, Inc., (ESRI), which produced Arc/Info, Arc View, and other products.
            CERL is also seeking agreements with Intergraph, MGE’s producer, and with Logiciels et Applications Scientifiques, Inc. (LAS),
            a Montreal company providing COTS software products (GRASSLANDS) running in a PC environment and based on GRASS.

            Under the planned agreements, CERL will take an active role in helping the industry partner understand the GIS needs at military installations
            and within the Army Corps of Engineers.
            In addition, CERL will continue developing advanced technology for geospatial modeling and specific applications related to land management.
            This will be cooperative development with COTS vendors to ensure all products will work with the systems used at military sites.
        -->
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you know GRASS GIS?
    </small>
</section>


<section>
    <h3>NSF Open-Source Ecosystem Grant</h3>
    <ul>
        <li>$1.5M NSF grant awarded to <b>NC State</b>, ASU, NMSU, Yale.</li>
        <li>To enhance infrastructure, revise contributing guidelines and to support community building of GRASS GIS.</li>
        <li>The NSF program is aiming at improving sustainability, not sustaining the project, adding features, or fixing bugs.</li>
    </ul>
    <img src="img/nsf_grant_announcement.png" class="stretch">
    <small class="glow">
        Discussion questions:
        Anything surprising in what is funded? Why? Why not?
    </small>
</section>


<section>
    <h2>Turn of the Tide?</h2>
</section>


<section>
    <h3>National Institutes of Health New 2023 Policy</h3>

    <blockquote>
    NIH expects that [...] researchers will maximize the appropriate sharing of scientific data, acknowledging certain factors (i.e., legal, ethical, or technical) [...]
    Shared scientific data should be made accessible as soon as possible [...]
    <small>
    <br>
    NOT-OD-21-013: Final NIH Policy for Data Management and Sharing. Retrieved November 6, 2024, from <a href="https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html">https://grants.nih.gov/grants/guide/notice-files/NOT-OD-21-013.html</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Would you expect a health-related organization to be on the forefront of sharing data?
    </small>
</section>


<section>
    <h3>Year of Open Science</h3>
    <blockquote>
        [The White House Office of Science and Technology Policy (OSTP)] is [...]
        launching the Year of Open Science, featuring actions across the federal
        government throughout 2023 to advance national open science policy,
        provide access to the results of the nation’s taxpayer-supported research,
        accelerate discovery and innovation, promote public trust,
        and drive more equitable outcomes.
        <small>
            <br>
            FACT SHEET: Biden-Harris Administration Announces New Actions to Advance Open and Equitable Research.
            January 11, 2023.
            <a href="https://www.whitehouse.gov/ostp/news-updates/2023/01/11/fact-sheet-biden-harris-administration-announces-new-actions-to-advance-open-and-equitable-research/">whitehouse.gov/...open-and-equitable-research</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Anything you consider new?
    </small>
</section>


<section>
    <h3>NASA's commitment</h3>
    <blockquote>
        [In 2022,] NASA committed $20 million per year to advance open science, beginning in 2023.
        <small>
            <br>
            Why NASA and federal agencies are declaring this the Year of Open Science.
            Nature 613, 217 (2023).
            <a href="https://doi.org/10.1038/d41586-023-00019-y">DOI 10.1038/d41586-023-00019-y</a>
        </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What do you think this will be spend on?
    </small>
</section>


<section>
    <h3>Nelson Memo: Peer-Reviewed Publications (2025)</h3>

    <blockquote>
        [...] all peer-reviewed scholarly publications [...] resulting from federally funded research are made freely
        available [...] without any [...] delay after publication.
    <small>
    <br>
    White House Office of Science and Technology Policy (2022). Desirable Characteristics of Data Repositories for Federally Funded Research. <a href="https://doi.org/10.5479/10088/113528">DOI 10.5479/10088/113528</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What open-science concept this refers to?
    </small>
</section>


<section>
    <h3>Nelson Memo: Scientific Data (2025)</h3>

    <blockquote>
        Scientific data underlying peer-reviewed scholarly publications resulting from
        federally funded research should be made freely available [...] at the time of publication, unless subject to limitations [...]
    <small>
    <br>
    White House Office of Science and Technology Policy (2022). Desirable Characteristics of Data Repositories for Federally Funded Research. <a href="https://doi.org/10.5479/10088/113528">DOI 10.5479/10088/113528</a>

    <small>
        [...] “scientific data” include the recorded factual material commonly accepted
        in the scientific community as of sufficient quality to validate and replicate research findings. Such scientific data do
        not include laboratory notebooks, preliminary analyses, case report forms, drafts of scientific papers, plans for future
        research, peer-reviews, communications with colleagues, or physical objects and materials, such as laboratory
        specimens, artifacts, or field notes.
    </small>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        What open-science concept or concepts this refers to?
    </small>
</section>


<section>
    <h3>Open Source Software and Industry</h3>

    <blockquote>
        Open source became a movement – a mentality. Suddenly infrastructure
        software was nearly free [comparing to 1999]. We paid 10% of the normal
        costs for the software and that money was for software support. A
        90% disruption in cost spawns innovation – believe me.
    <small>
        <br>
        &mdash; Mark Suster (2011) in
        <br>
        Eghbal, Nadia (2016). Roads and bridges: The unseen labor behind our digital infrastructure. <a href="https://www.fordfoundation.org/work/learning/research-reports/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure/">Ford Foundation</a>
    </small>
    </blockquote>

    <small class="glow">
        Discussion questions:
        Do you know any open-source software success stories?
    </small>
</section>


<section>
    <h2>Other Reasons for Doing Open Science</h2>
</section>


<section>
<h3>Open Science Beginnings</h3>

<p>
First journal ever published:<br>
<em>Philosophical Transactions (of the Royal Society)</em>
</p>

<img src="img/philosophical_transactions.jpg" class="stretch">
<p class="credit glow">
    CC BY Stefan Janusz,
    <a href="https://en.wikipedia.org/wiki/File:Philosophical_Transactions_Volume_1_frontispiece.jpg">Wikipedia</a>
</p>
</section>


<section>
<h3>Theoretical Publishing Goals</h3>
<ul>
    <li>registration <small>so that scientists get credit</small>
    <li>archiving <small>so that we preserve knowledge for the future</small>
    <li>dissemination <small>so that people can use this knowledge</small>
    <li>peer review <small>so that we know it's worth it</small>
</ul>

<p>
<small>
    Discussion questions:
    <!-- How are these goals different from goals of science? -->
    How are these publishing goals fulfilled by journal papers?
</small>
</section>


<section>
    <h3>Internal Reasons for Open Science</h3>

    <ul>
        <li>Open science in your lab (team-oriented reasons):
            <ul>
                <li>collaboration <small>work together with your colleagues</small>
                <li>transfer <small>transfer research between you and your colleague</small>
                <li>longevity <small>re-usability of parts of research over time</small>
            </ul>
        <li>Open science by yourself (“selfish” reasons):
            <ul>
                <li>revisit <small>revisit or return to a project after some time</small>
                <li>correction <small>correct a mistake in the research</small>
                <li>extension <small>improve or build on an existing project</small>
            </ul>
    </ul>

    <p>
    <small>
        Discussion questions:
        What is your experience with getting back to your own research
        or continuing research started by someone else?
        <small>(See <a href="http://phdcomics.com/comics.php?f=1689">PhD Comics: Scratch</a>.)</small>
        How does open science relate to team science?
        How making things public can help us to achieve the desired effect and what challenges that brings?
    </small>

</section>


<section>
    <h2>Open Science Components and Definitions</h2>
</section>


<section>
    <h3>What Open Means</h3>

    <!-- TODO: links -->
    <ul>
        <li>The Open Definition
            <ul>
                <li>Knowledge is open if anyone is free to access, use, modify,
                    and share it — subject, at most, to measures
                    that preserve provenance and openness.
                    <small>[as officially summed up]</small>
            </ul>
        <li>More than one term in use: <em>open, free, libre</em>
        <li>Free Cultural Works
        <li>The Open Source Definition
        <li>The Free Software Definition
        <li>The Debian Free Software Guidelines and the Debian Social Contract
    </ul>

    <br>
    <img class="stretch" src="img/free_beer_bottles.jpg">
    <p class="credit">
        <small>
            Image: “Free beer bottles” by <a href="https://en.wikipedia.org/wiki/File:Free_beer_bottles.jpg">free beer pool</a> (CC BY 2.0)
        </small>
    </p>

    <p>
    <small class="glow">
        Discussion questions:
        What is the difference between “free as in free beer” and “free as in freedom”?
        Have you seen “open” being used for something not fulfilling the Open Definition?
    </small>
</section>

<section>
<h3>Open Science Components</h3>

<div class="fragment">

<ul class="left">
<li>6 pillars <small>[Watson 2015]</small>:
    <ul>
        <li>open methodology
        <li>open access
        <li>open data
        <li>open source
        <li>open peer review
        <li>open education (or educational resources)
    </ul>
<li>other components:
    <ul>
        <li>open hardware
        <li>open formats
        <li>open standards
    </ul>
</ul>

<ul class="right">
<li>related concepts:
    <ul>
        <li>Open-notebook science
        <li>Provenance
        <li>FAIR Principles
        <li>Science 2.0 <small>(like Web 2.0)</small>
        <li>Team science
        <li>Citizen science
        <li>Public science
        <li>Participatory research
        <li>Open innovation
        <li>Open organization
        <li>Crowdsourcing
        <li>Preprints
        <li>Inner source
    </ul>
</ul>

<p>
<small>
    Discussion questions:
    What would add to the list?
    What do you see something for the first time?
</small>

</div>

</section>

<!--
Watson, M. (2015). When will ‘open science’ become simply ‘science’?.
Genome biology, 16(1), 101. doi:10.1186/s13059-015-0669-2
There are six commonly accepted pillars of open science: open data,
open access, open methodology, open source, open peer review and open education.
-->


<section>
    <h3>The “re” Words</h3>

    <small>
    No agreement on some of the definitions especially in different fields;
    definitions are often overlapping or swapped, some don't make any distinction.
    </small>

    <ul>
        <li>replicability <small>independent validation of specific findings</small>
        <li>repeatability <small>same conditions, people, instruments, ... (test–retest reliability)</small>
        <li>reproducibility <small>same results using the same raw data or same materials, procedures, ...</small>
        <li>recomputability <small>same results received by computation (in computational research)</small>
        <li>reusability <small>use again the same data or methods</small>
    </ul>

    <small>
    For example, Ince et al. (2012) in computational science
    distinguishes <em>direct reproducibility</em> as rerunning the code and
    <em>indirect reproducibility</em> as validating something other than the entire code.
    </small>

    <br>
    <img class="stretch" src="img/recycle.png">

    <!--
        Openclipart, Public Domain, Green recycling, garethclubb
        https://openclipart.org/detail/171146/green-recycling
    -->

    <br>
    <small class="glow">
        Discussion questions:
        As a PhD student, which of these features would you like to see in other research?
        As a journal paper reviewer, what should you be able to do
        when you receive a scientific publication for review?
    </small>

    </section>


<section>
<h3>Computational and Geospatial Research</h3>

<ul>
    <li>
        code is a part of method description
        <small>[Ince et al. 2012, Morin et al. 2012, Nature Methods 2007]</small>
    <li>
        use of open source tools is a part of reproducibility
        <small>[Lees 2012, Alsberg &amp; Hagen 2006]</small>
    <li>
        <em>easily reproducible result</em> is a result obtained in 10 minutes
        <small>[Schwab et al. 2000]</small>
    <li>
        geospatial research specifics:
        <ul>
            <li>some research introduces new code
            <li>some research requires significant dependencies
            <li>some research produces user-ready software
        </ul>
</ul>

<p>
<small>
    Discussion questions:
    Is spatial special?
    Is recomputing the results useful for research?
    How long should it take to recompute results?
    Do dependencies need to be open source as well?
</small>

</section>


<section>
    <h3>Open Science Publication: Use Case</h3>
    Petras et al. 2017
    <br>
    <img class="stretch" src="img/petras2017generalized_full.png">
    <br>
    <small class="credit">
        Petras, V., Newcomb, D. J., &amp; Mitasova, H. (2017).
        <em>Generalized 3D fragmentation index derived from lidar point clouds.</em>
        In: Open Geospatial Data, Software and Standards 2(1), 9.
        <a href="https://doi.org/10.1186/s40965-017-0021-8">DOI 10.1186/s40965-017-0021-8</a>
    </small>
</section>


<section>
<h3>Open Science Publication: Components</h3>
<table style="font-size: smaller;">
    <tr>
        <th>Publication Component</th>
        <th>in the Petras et al. 2017 use case</th>
    <tr>
        <td>Text</td>
        <td>
            background, methods, results, discussion, conclusions, &hellip; <small>(OA)</small>
        </td>
    </tr>
    <tr>
        <td>Data</td>
        <td>
            input data <small>(formats readable by open source software)</small>
        </td>
    </tr>
    <tr>
        <td>Reusable&nbsp;Code</td>
        <td>
            methods as GRASS GIS modules <small>(C &amp; Python)</small>
        </td>
    </tr>
    <tr>
        <td>Publication-specific&nbsp;Code</td>
        <td>
            scripts to generate results <small>(Bash &amp Python)</small>
        </td>
    </tr>
    <tr>
        <td>Computational Environment</td>
        <td>
            details about all dependencies and the code <small>(Docker, Dockerfile*)</small>
        </td>
    </tr>
    <tr>
        <td>Versions</td>
        <td>
            repository with current and previous versions* <small>(Git, GitHub)</small>
        </td>
    </tr>
</table>
<p>
    <small style="font-size: 70%;">
    * Version associated with the publication included also as a supplemental file.
    </small>
</p>

<small class="credit">
Petras, V. (2018). <em>Geospatial analytics for point clouds
in an open science framework</em>. Doctoral dissertation.
<a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">URI&nbsp;http://www.lib.ncsu.edu/resolver/1840.20/35242</a>
</small>

<p>
<small>
    Discussion questions:
    What are other technologies which are good fit for these components?
    Are there other components or categories?
    What parts of research did you publish or tried to publish and what challenges did you face?
</small>

</section>


<section>
    <h3>Open Science Publication: In a Single Package Online</h3>
    <ul>
        <li>Components other than Text and Versions for Petras et al. 2017 are now also available at <a href="https://doi.org/10.24433/CO.3986355.v2" title="Goes to the capsule">Code Ocean</a> as a capsule.
    </ul>

    <img class="stretch" style="margin: 0px"; src="img/code_ocean_petras_2017.png" alt="Code Ocean capsule content in a web browser">
    <br>
    <small class="credit">
    <a href="https://doi.org/10.24433/CO.3986355.v2">DOI&nbsp;10.24433/CO.3986355.v2</a>
    </small>

    <p class="glow">
    <small>
        Discussion questions:
        What is the skill set needed to publish results like this?
        What is the long-term sustainability of online recomputability tools such as Code Ocean?
    </small>
</section>


<section>
    <h3>Open Science Publication: Software Platform</h3>
    <ul>
        <li>Preprocessing, visualization, and interfaces <small>(GUI, CLI, API)</small>
        <li>Data inputs and outputs, memory management
        <li>Integration with existing analytical tools
        <li>Preservation of the reusable code component <small>(long-term maintenance)</small>
        <li>Dependency which would be hard to change for something else
        <li>Example: FUTURES model implemented as a set of GRASS GIS modules
            <small>(<em>r.futures.pga, r.futures.demand, r.futures.parallelpga, ...</em>)</small>
    </ul>

    <small class="credit">
        Petras, V. (2018). <em>Geospatial analytics for point clouds
        in an open science framework</em>. Doctoral dissertation.
        <a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">URI&nbsp;http://www.lib.ncsu.edu/resolver/1840.20/35242</a>
    </small>

    <p class="glow">
    <small>
        Discussion questions:
        What software can play this role?
        What are the different levels of integration with a piece of software and their advantages and disadvantages?
    </small>
</section>


<section>
<h3>Licensing</h3>

<ul class="left">
    <li>Creative Commons (CC) licenses
    <li>GNU licenses, BSD licenses, ...
        <ul>
            <li>e.g., GNU GPL 3.0
        </ul>
    <li>Choosing a license and lists of licenses:
        <ul>
            <li><a href="https://choosealicense.com/">choosealicense.com</a> <small>(for software)</small>
            <li><a href="https://help.data.world/hc/en-us/articles/115006114287-Common-license-types-for-datasets">help.data.world/...</a> <small>(for data)</small>
            <li><a href="https://creativecommons.org/choose/">creativecommons.org/choose</a> <small>(CC licenses; for anything except software)</small>
            <li><a href="http://ufal.github.io/public-license-selector/">License Selector</a> <small>(software and CC licenses)</small>
            <li><a href="https://www.openaire.eu/research-data-how-to-license/">OpenAIRE</a> <small>(data license Q&amp;A)</small>
            <li><a href="https://opendatacommons.org/">Open Data Commons</a>
            <li><a href="https://opensource.org/licenses">Open Source Initiative</a>
            <li><a href="https://www.gnu.org/licenses/license-list.html">GNU (Free Software Foundation)</a>
        </ul>
</ul>

<div class="right">
<img style="max-height: 15em !important;" src="img/cc_license_spectrum.png">
<p class="credit">
    “Creative Commons License Spectrum” by <a href="https://commons.wikimedia.org/wiki/File:Creative_commons_license_spectrum.svg">Shaddim</a> (CC BY 4.0),
    <a href="https://creativecommons.org/share-your-work/public-domain/freeworks">Creative Commons: Understanding Free Cultural Works</a>
</p>
</div>

<p>
<small class="glow">
    Discussion questions:
    Do you read “terms and conditions”?
    Have you ever read any “terms and conditions” or end user license agreement (EULA)?
    What about an open source software license?
    <small>(Read <a href="https://trac.osgeo.org/gdal/wiki/FAQGeneral#WhatlicensedoesGDALOGRuse">license of GDAL</a> right now! It's less than 170 words.)</small>
</small>

</section>


<section>
    <h3>FAIR Principles</h3>

    <ul>
        <li>Findable <small>persistent identifier and metadata for data</small>
        <li>Accessible <small>sharing protocol is open, free, and universally implementable</small>
        <li>Interoperable <small>formal language and references to other datasets</small>
        <li>Reusable <small>clear usage license (can be restricted), detailed provenance</small>
    </ul>

    <p>
        <em>
            The principles emphasize machine-actionability (&hellip;)
            because humans increasingly rely on computational support to deal with data (&hellip;)
        </em>
    </p>

    <small class="credit">
        [Wilkinson 2016]
        <br>
        <a href="https://www.go-fair.org/fair-principles/">https://www.go-fair.org/fair-principles</a>
    </small>

    <br>
    <img class="stretch" src="img/fair_data_principles.jpg">
    <p class="credit">
        <small>
            Image: “FAIR guiding principles for data resources” by <a href="https://commons.wikimedia.org/wiki/File:FAIR_data_principles.jpg">SangyaPundir</a> (CC BY-SA 4.0)
        </small>
    </p>

    <p class="glow">
    <small>
        Discussion questions:
        Which parts are unique to FAIR and not present in open science?
        Is source code part of data, data provenance, or it is a separate thing?
    </small>
</section>


<section>
<h3>References</h3>

<small style="font-size: 50%" class="glow">

<ul>
<li>
Alsberg, Bjørn K., and Ole Jacob Hagen. “How Octave Can Replace Matlab in Chemometrics.” Chemometrics and Intelligent Laboratory Systems, Selected papers presented at the 9th Scandinavian Symposium on Chemometrics Reykjavik, Iceland 21–25 August 2005, 84, no. 1 (December 1, 2006): 195–200. <a href="https://doi.org/10.1016/j.chemolab.2006.04.025">doi:10.1016/j.chemolab.2006.04.025</a>.
<li>
Buckheit, Jonathan B., and David L. Donoho. “WaveLab and Reproducible Research.” In Wavelets and Statistics, edited by Anestis Antoniadis and Georges Oppenheim, 103:55–81. Lecture Notes in Statistics. New York, NY: Springer New York, 1995. <a href="https://doi.org/10.1007/978-1-4612-2544-7_5">doi:10.1007/978-1-4612-2544-7_5</a>.
<li>
Ince, Darrel C., Leslie Hatton, and John Graham-Cumming. “The case for open computer programs”. In: Nature 482.7386 (2012), pp. 485–488. <a href="http://doi.org/10.1038/nature10836">doi:10.1038/nature10836</a>
<li>
Lees, Jonathan M. “Open and free: Software and scientific reproducibility”. In: Seismological Research Letters 83.5 (2012), pp. 751–752. <a href="http://doi.org/10.1785/0220120091">doi:10.1007/s10816-015-9272-9</a>
<li>
Marwick, Ben. “Computational reproducibility in archaeological research: basic principles and a case study of their implementation”. In: Journal of Archaeological Method and Theory 24.2 (2017), pp. 424–450. <a href="http://doi.org/10.1007/s10816-015-9272-9">doi:10.1007/s10816-015-9272-9</a>
<li>
Morin, A et al. “Shining light into black boxes”. In: Science 336.6078 (2012), pp. 159–160. <a href="http://doi.org/10.1126/science.1218263">doi:10.1126/science.1218263</a>
<li>
Nature Publishing Group. “Social Software.” Nature Methods 4, no. 3 (March 2007): 189. <a href="https://doi.org/10.1038/nmeth0307-189">doi:10.1038/nmeth0307-189</a>.
<li>
Peng, Roger D. “Reproducible Research in Computational Science.” Science (New York, N.Y.) 334, no. 6060 (December 2, 2011): 1226–27. <a href="https://doi.org/10.1126/science.1213847">doi:10.1126/science.1213847</a>
<li>
Petras, Vaclav. “Geospatial analytics for point clouds in an open science framework.” Doctoral dissertation. 2018. <a href="http://www.lib.ncsu.edu/resolver/1840.20/35242">www.lib.ncsu.edu/resolver/1840.20/35242</a>
<li>
Petras, Vaclav, Douglas J. Newcomb, and Helena Mitasova. “Generalized 3D Fragmentation Index Derived from Lidar Point Clouds.” Open Geospatial Data, Software and Standards 2, no. 1 (April 2017): 9. <a href="https://doi.org/10.1186/s40965-017-0021-8">doi:10.1186/s40965-017-0021-8</a>.
<li>
Rocchini, Duccio and Markus Neteler. “Let the four freedoms paradigm apply to ecology”. In: Trends in Ecology and Evolution (2012). <a href="http://doi.org/10.1016/j.tree.2012.03.009">doi:10.1016/j.tree.2012.03.009</a>
<li>
Rodriguez-Sanchez, Francisco, Antonio Jesús Pérez-Luque, Ignasi Bartomeus, and Sara Varela. “Ciencia Reproducible: Qué, Por Qué, Cómo.” Revista Ecosistemas 25, no. 2 (2016): 83–92.
<li>
Schwab, Matthias, Martin Karrenbach, and Jon Claerbout. “Making Scientific Computations Reproducible.” Computing in Science & Engineering 2, no. 6 (2000): 61–67. <a href="https://doi.org/10.1109/5992.881708">doi:10.1109/5992.881708</a>.
<li>
Stodden, V., Seiler, J., &amp; Ma, Z. (2018). “An empirical analysis of journal policy effectiveness for computational reproducibility.” In: <em>Proceedings of the National Academy of Sciences</em> 115(11), p. 2584-2589. <a href="https://doi.org/10.1073/pnas.1708290115">doi:10.1073/pnas.1708290115</a>
<li>
Watson, M. (2015). When will ‘open science’ become simply ‘science’?. Genome biology, 16(1), 101. <a href="https://doi.org/10.1186/s13059-015-0669-2">doi:10.1186/s13059-015-0669-2</a>
<li>
Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., ... &amp; Bouwman, J. (2016).
“The FAIR Guiding Principles for scientific data management and stewardship”. <em>Scientific data</em>, 3(1), 1-9.
<a href="https://doi.org/10.1038/sdata.2016.18">doi:10.1038/sdata.2016.18</a>
</ul>

</small>

</section>


<section>
    <h3>Resolution for Class Debate</h3>

    <blockquote>
        A scientific publication needs to consist of text, data, source code, software environment, and reviews
        which are all openly licensed, in open formats, checked during the submission process, and
        publicly available without any delay after publication.
    </blockquote>

</section>
